{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WEzJ2uPcHdS"
   },
   "source": [
    "## Part 0: Demonstration\n",
    "\n",
    "This section loads and highlights the finalized trained model from this notebook, and allows usage for sample sentences or inputs for visuaolizing the results."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets transformers evaluate arabert\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TROtjPwBU1kn",
    "outputId": "a509bd46-4d73-4da5-d685-96ce24082302"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "model_path = \"/content/best_model.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer (same as training)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Recreate model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_arabert(text, max_len=128):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "    label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = probs[0][pred].item()\n",
    "\n",
    "    return label, confidence\n"
   ],
   "metadata": {
    "id": "7vQjsMr-AapF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719,
     "referenced_widgets": [
      "ba4949b2443e432bbf10b882ac92479d",
      "35b2a54c072d4ca5a665cc9f4803b774",
      "5dbbfa09f82f4436b62c357903f457e6",
      "6f90b605734846e39c6223b5be23e636",
      "a7c9aac0ffd9407f84302a5ac283755d",
      "c93051e41e4145f4b393b777824ac6de",
      "4bb3409c65934f98b36d330a33c84b5c",
      "777d5193118b438c9560f38cf3c6fe62",
      "65998143d5a547558ccbc80d6a7caeea",
      "ef9383eefdf747499aa10ca114d015fb",
      "d822e374d9c7460190ec815eaf490f2d",
      "92babfa7f7fe49448ac4a36d8df52824",
      "9326cf4ab8364f0dbd6e3eb09c8bb9ff",
      "fcd18213ac0e43aea55680a748bdd1ba",
      "1aaac1bf953942efa2889d021027ea6e",
      "1138e64e3ea94014a3b029c24d32cae2",
      "90713f60f932437cb86ffc1efd7fcb46",
      "a64f3f2fe7614a2099bf114aa77eee41",
      "937058a9fcb3442c8399713361b754ea",
      "dc02ac2970184c8b85f36536cecb6522",
      "f8b402032cb54d39af365b0e98e060ce",
      "4ee13bab906f4ce2acf72ecb081abca7",
      "bad94b5f07224e09ab92296502fadfdb",
      "601d6bc783924c16924340ecc908176e",
      "d9d6fa65095a47058a0654575b152d6d",
      "c41a1bd93a29402ba0a9c4f8cf7ec5e3",
      "eb1a2a0bd01940d9bc48230b991c7de9",
      "7edd2aeb774f4e12826d80040089d1bc",
      "10b0e186bbf947478ffb630b0c65ed43",
      "8f2742bf4ee243508d22bab49fe783df",
      "df84b60656df443e98f42f6a3d07a713",
      "df16d22a46c6442ab433c1125a1a83c3",
      "8df87b7514294067839abfb44b573cc2",
      "190d0b8fd11444afa0595b8d4c28a625",
      "b92bfd3803674f938d227fa1e3e9e535",
      "0a9e5b4622b44335bd872173670d7281",
      "43b9581f410c4325a82166704de3c157",
      "f92c07b72ba241a3a19a2de0c2e05cbd",
      "81f00402d4034f848ea7e60cb4c5b6a6",
      "64236e1497a7474bbd295fe827fafc8b",
      "e6a848d6a8c24cae974019de363000d6",
      "edc3c967e3f74ee992a7651e752a4782",
      "982ef07094734bd089bb2e4de97d8aa8",
      "f2cdd4df8bb84652a547c166ac103fdb",
      "66ab6447dd9a4ca8816ba3190fb56a9e",
      "4b50e53e33d04bbd8b1c7fd7337a4be7",
      "77f4a40e6dbe4e1684e5bb2bd9ba54c9",
      "a329e1708d71480daab0cdfe9d9d9dbf",
      "e31bf083f18b4a10b58a258ef797bbc3",
      "e86f4a4fefb5409b9d5e18eee227c91e",
      "9e221b8ee71e4ab99b174f187fdce723",
      "04a1e12c9cb04467acce17590f1f6c7a",
      "d75f4a4da01043a79ca2c2835b1c49da",
      "bb0796d1e65344229562cce958aa79f7",
      "6b4943890ac240a0a1d3b6604a8374af",
      "7110d14bf2cf4655a315aa2804c7e9f0",
      "1b4cf4cba9d349e8b960e238ae2f01b3",
      "ca5dd6e02aa24de58717c1eda83c2ac1",
      "438c418adb314f1fbc991a582d58989c",
      "5f572fa5126547a29df6f3f8e7edf2fc",
      "d0285a251a75480bb07d3a1457105546",
      "60fcbff99fbd480188523026e48f09d6",
      "b683c161db114929b6f488819b8b76fe",
      "5e284859100c4697b32dcfa987ceac26",
      "0a0b567f57384a8e8009eec30f381748",
      "a1234c10a4524622b594c2d6e9356bb0"
     ]
    },
    "outputId": "e39eaac3-bcbb-4682-feda-3447cbb4c48b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJqVlyj3cU08"
   },
   "outputs": [],
   "source": [
    "example = \"هذا رائع جدا\"\n",
    "\n",
    "# SVM Model\n",
    "bundle = joblib.load(\"svm_tfidf_pipeline.joblib\")\n",
    "\n",
    "svm_model = bundle[\"model\"]\n",
    "tfidf_vectorizer = bundle[\"vectorizer\"]\n",
    "\n",
    "X = tfidf_vectorizer.transform([example])\n",
    "print(\"SVM: \", \"Positive\" if (svm_model.predict(X) == 1)[0] == 1 else \"Negative\")\n",
    "\n",
    "# Linear Regresion Model\n",
    "bundle = joblib.load(\"linear_regression_tfidf.joblib\")\n",
    "\n",
    "linear_regression = bundle[\"model\"]\n",
    "tfidf_vectorizer = bundle[\"vectorizer\"]\n",
    "\n",
    "X = tfidf_vectorizer.transform([example])\n",
    "print(\"Linear Regression: \", \"Positive\" if (linear_regression.predict(X) > 1)[0] else \"Negative\")\n",
    "\n",
    "# AraBERT Model\n",
    "label, confidence = predict_arabert(example)\n",
    "\n",
    "print(f\"AraBERT: {label} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyqpUN3KLLky"
   },
   "source": [
    "## Part 1: Dependencies and Imports\n",
    "\n",
    "This section installs the necessary libraries and imports them for use in this colab notebook. It also defines high-level constants that will be used for hyperparameters in training the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tQdQ7KY7Yl11",
    "outputId": "64c440c1-c28b-4331-8316-aeb2087abbc3"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate arabert\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAL9n1t9UqFZ"
   },
   "outputs": [],
   "source": [
    "max_epochs = 12\n",
    "learning_rate = 1.5e-5\n",
    "weight_decay = 0.01\n",
    "scheduler_factor = 0.1\n",
    "scheduler_patience = 1\n",
    "patience = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7ffumUYMsbN"
   },
   "source": [
    "## Part 2: Load and Process Dataset\n",
    "\n",
    "This section will load the dataset and apply various processing and normalization techniques for the dataset to be ready for use in the ML / NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Sedl1lnvNyZH",
    "outputId": "cfcb07e7-1caf-4cfb-8d1f-71839777b006"
   },
   "outputs": [],
   "source": [
    "positive_samples = pd.read_csv(\"test.tsv\", sep='\\t', header=None, names=['label', 'text'])\n",
    "negative_samples = pd.read_csv(\"train.tsv\", sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "df = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "df['label'] = df['label'].map({'neg': 0, 'pos': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(df)"
   ],
   "metadata": {
    "id": "3LEkY0F5f26i",
    "outputId": "9505e79d-c7a1-44a7-9ce3-515be4288ef2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPtl6sCWOeVj",
    "outputId": "7cc44ef6-c98a-4cac-e4fc-740fc5fac77a"
   },
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def normalize_arabic_light(text):\n",
    "    # Replace underscores with spaces\n",
    "    text = text.replace('_', ' ')\n",
    "\n",
    "    # Remove diacritics only\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "    # Normalize Alef variants\n",
    "    text = re.sub(r'[إأآ]', 'ا', text)\n",
    "\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def arabert_normalize(text):\n",
    "    text = normalize_arabic_light(text)\n",
    "    text = arabert_prep.preprocess(text)\n",
    "    return text\n",
    "\n",
    "df['text_normalized'] = df['text'].apply(arabert_normalize)\n",
    "\n",
    "print(df[['text', 'text_normalized']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(\"BEFORE :\", df.loc[i, 'text'])\n",
    "    print(\"AFTER  :\", df.loc[i, 'text_normalized'])\n",
    "    print(\"-\" * 80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrczSM0PMB2_",
    "outputId": "9e2c7814-5422-4abf-9140-492bcde06291"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEkSUAxCO26i"
   },
   "source": [
    "## Part 3: Feature Extraction (TF-IDF)\n",
    "\n",
    "This section will generate the necessary features to train the classic ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SF3mnXD1QXB7"
   },
   "outputs": [],
   "source": [
    "X = df['text_normalized'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=30000,\n",
    "    min_df=3,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf   = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU6N6PcmTGcM"
   },
   "source": [
    "## Part 4: Classic ML Models\n",
    "\n",
    "This section will train the classic ML models: linear regression + SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfAKF5EMIY5I",
    "outputId": "a0ec26d0-3e25-47a4-8669-871792a5724e"
   },
   "outputs": [],
   "source": [
    "# Train SVM model (linear SVC)\n",
    "svm_model = LinearSVC(\n",
    "    C=5.0,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=20000\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Validation\n",
    "val_preds = svm_model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
    "print(classification_report(y_val, val_preds, digits=4))\n",
    "\n",
    "# Test\n",
    "test_preds = svm_model.predict(X_test_tfidf)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cjm3tr1NXYgy",
    "outputId": "e45e13b3-f3ed-4d7d-c9cf-8a6a0ccfdee8"
   },
   "outputs": [],
   "source": [
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": svm_model,\n",
    "        \"vectorizer\": tfidf_vectorizer\n",
    "    },\n",
    "    \"svm_tfidf_pipeline.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qX3h825RDNj",
    "outputId": "6537609d-2b87-4dfc-ab7a-cd0b82fb19b3"
   },
   "outputs": [],
   "source": [
    "df['text_normalized'] = df['text'].apply(normalize_arabic_light)\n",
    "\n",
    "X = df['text_normalized'].tolist()\n",
    "y = [1 if label == 1 else -1 for label in df['label']]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=30000,\n",
    "    min_df=3,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_class = [1 if p >= 0 else -1 for p in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(classification_report(y_test, y_pred_class, digits=4))\n",
    "\n",
    "test_preds = svm_model.predict(X_test_tfidf)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_class))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FepFVFF81yWT",
    "outputId": "0650a233-c2dc-4568-b8f7-9e535ed485a9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dJQIG0iXlWX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e23538e9-40f9-45ce-be6e-910ae948ef3b"
   },
   "outputs": [],
   "source": [
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": model,\n",
    "        \"vectorizer\": tfidf_vectorizer\n",
    "    },\n",
    "    \"linear_regression_tfidf.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ty4lwTEVH8t"
   },
   "source": [
    "## Part 5: AraBERT Transformer Based LLM / NLP Model\n",
    "\n",
    "This section will focus on fine-tuning an AraBERT model and training an additional classification layer on top of the encoder to predict the two outcomes (positive / negative sentiment) of the given entry in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Nd95v6Ud_kCA",
    "outputId": "e29c4e22-f5a7-42d0-9d90-62d20a4aaa65"
   },
   "outputs": [],
   "source": [
    "# Initialize Model Tokenizer\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Split to train, validate, test dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_labels\n",
    ")\n",
    "\n",
    "train_texts = list(train_texts)\n",
    "train_labels = list(train_labels)\n",
    "val_texts = list(val_texts)\n",
    "val_labels = list(val_labels)\n",
    "test_texts = list(test_texts)\n",
    "test_labels = list(test_labels)\n",
    "\n",
    "# Convert to Pytorch Dataset\n",
    "class ArabicSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ArabicSentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset   = ArabicSentimentDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset  = ArabicSentimentDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "t8jbfmLI-LlO",
    "outputId": "acd7867f-00f9-454c-a43a-aca36c914d13"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.bert.encoder.layer[-6:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "collapsed": true,
    "id": "rHEyNbzD-So_",
    "outputId": "8795d7f6-6574-4e9e-e4e6-e2d5aed2e47f"
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_loader) * max_epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Warm-up scheduler (per batch)\n",
    "warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau (per epoch)\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=scheduler_factor,\n",
    "    patience=scheduler_patience,\n",
    ")\n",
    "\n",
    "def train():\n",
    "  best_val_loss = float(\"inf\")\n",
    "  patience_counter = 0\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "      model.train()\n",
    "      train_loss = 0\n",
    "\n",
    "      # Training\n",
    "      for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs} [Train]\"):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          input_ids = batch[\"input_ids\"].to(device)\n",
    "          attention_mask = batch[\"attention_mask\"].to(device)\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          outputs = model(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              labels=labels\n",
    "          )\n",
    "\n",
    "          loss = outputs.loss\n",
    "          loss.backward()\n",
    "\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "          optimizer.step()\n",
    "          warmup_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_loss += loss.item()\n",
    "\n",
    "      train_loss /= len(train_loader)\n",
    "\n",
    "      # Validation\n",
    "      model.eval()\n",
    "      val_loss = 0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{max_epochs} [Val]\"):\n",
    "              input_ids = batch[\"input_ids\"].to(device)\n",
    "              attention_mask = batch[\"attention_mask\"].to(device)\n",
    "              labels = batch[\"labels\"].to(device)\n",
    "\n",
    "              outputs = model(\n",
    "                  input_ids=input_ids,\n",
    "                  attention_mask=attention_mask,\n",
    "                  labels=labels\n",
    "              )\n",
    "\n",
    "              val_loss += outputs.loss.item()\n",
    "              preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "              correct += (preds == labels).sum().item()\n",
    "              total += labels.size(0)\n",
    "\n",
    "      val_loss /= len(val_loader)\n",
    "      val_acc = correct / total\n",
    "\n",
    "      print(\n",
    "          f\"\\nEpoch {epoch+1}: \"\n",
    "          f\"Train Loss = {train_loss:.4f} | \"\n",
    "          f\"Val Loss = {val_loss:.4f} | \"\n",
    "          f\"Val Acc = {val_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "      # Early Stopping\n",
    "      if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          patience_counter = 0\n",
    "          torch.save(model.state_dict(), \"best_model.pt\")\n",
    "      else:\n",
    "          patience_counter += 1\n",
    "          if patience_counter >= patience:\n",
    "              print(\"\\n⏹ Early stopping triggered\")\n",
    "              break\n",
    "\n",
    "      plateau_scheduler.step(val_loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYn8-BLL-UW_",
    "outputId": "f688f821-6171-4c99-968d-a868cd2cc2a2"
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "  model.eval()\n",
    "  correct, total = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for batch in test_loader:\n",
    "          input_ids = batch[\"input_ids\"].to(device)\n",
    "          attention_mask = batch[\"attention_mask\"].to(device)\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          outputs = model(input_ids, attention_mask=attention_mask)\n",
    "          preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "          correct += (preds == labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  print(\"Accuracy:\", correct / total)\n",
    "\n",
    "eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}